from functools import partial
from guesslet.prompts.cqa_prompt import PromptDict, CommonSenseQAPrompt
from guesslet.prompts.few_shot import FewShotPreppender
from transformers import PreTrainedTokenizer, AutoTokenizer, AutoModelForCausalLM
from transformers import BitsAndBytesConfig, DataCollatorForLanguageModeling
from datasets import load_dataset
import torch
from torch.utils.data import DataLoader
import torch.nn.functional as F
from tqdm.auto import tqdm
import numpy as np


def mistral_tokenize(sample: PromptDict, tokenizer: PreTrainedTokenizer, max_length=None):
    """Tokenizes the `prompt` field of a data sample.

    This function tokenizes the `prompt` field from a given data sample using a specified tokenizer.
    It optionally truncates the input to a maximum length. The function returns a dictionary
    containing `input_ids` and `attention_mask` generated by the tokenizer.

    Args:
        sample (dict): A dictionary representing a data sample. This dictionary must have
                       a key named "prompt" which contains the text to be tokenized.
        tokenizer (AutoTokenizer): An instance of `AutoTokenizer` from the Hugging Face Transformers
                                   library, pre-initialized and ready to use for tokenization.
        max_length (int, optional): The maximum length of the tokenized output. If the text
                                    exceeds this length, it will be truncated to fit.
                                    Defaults to None, in which case no truncation is performed.

    Returns:
        dict: A dictionary with two keys, `input_ids` and `attention_mask`, containing the
              tokenized output from the tokenizer. The exact structure of this dictionary
              (including any additional keys) might depend on the specific tokenizer used.

    Note:
        The `return_length` argument in the tokenizer call is set based on the presence of
        `max_length`. If `max_length` is not None, `truncation` is set to True, and
        `max_length` is enforced. Otherwise, tokenization proceeds without truncation, and
        `return_length` is set to False, which is not a valid tokenizer argument and might
        need correction based on the intended functionality.
    """
    if max_length:
        return tokenizer(sample["prompt"], truncation=True, max_length=max_length)
    # Corrected the return statement to exclude `return_length` which is not a valid argument
    return tokenizer(sample["prompt"])


def main():
    RANDOM_SEED = 38
    dataset = load_dataset("tau/commonsense_qa")
    tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
    prompter = CommonSenseQAPrompt()

    # Few Shot
    rng = np.random.default_rng(seed=RANDOM_SEED)
    few_shot_indexes = rng.choice(dataset["train"].num_rows, size=3, replace=False)
    fewshot_examples = (
        dataset["train"]
            .shuffle(seed=RANDOM_SEED)
            # Select the quantity of few shot examples
            .select(few_shot_indexes)
            # Create prompt string from structured data
            .map(prompter, desc="Prompt Creation: ")
    )

    few_shot_preppeder = FewShotPreppender(examples=fewshot_examples['prompt'])

    # Prepare inference Dataset
    inference_dataset = (
        dataset["validation"]
        # Create prompt string from structured data
        .map(prompter, desc="Prompt Creation: ")
        # Prepend few shot strings
        .map(few_shot_preppeder, desc="Few Shot Prepend: ")
        .map(partial(mistral_tokenize, tokenizer=tokenizer),
            batched=True,
            remove_columns=[
                'id',
                'question',
                'question_concept',
                'choices',
                'answerKey'
            ],
            desc="Tokenize: "
        )
    )

    # Load Model
    # Pretrained Mistral loading

    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16
    )

    model_path = "mistralai/Mistral-7B-v0.1"
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        quantization_config=bnb_config,
        device_map='auto',
    )


    tokenizer.pad_token = tokenizer.eos_token
    collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
    inference_dataloader = DataLoader(
        inference_dataset.map(None, remove_columns=['prompt']), 
        batch_size=16, 
        collate_fn=collator, 
        shuffle=False
    )

    token_ids = tokenizer.convert_tokens_to_ids(["a", "b", "c", "d", "e"])
    token_id_to_assigned_id = {
        token_id: assigned_id
        for assigned_id, token_id in enumerate(token_ids)
    }

    pred_labels = []
    truth_labels = []
    nlls = []
    for batch in tqdm(inference_dataloader):

        with torch.no_grad():
            outputs = model(**batch)
            # -3 is the place where the answer token is located
            logits = outputs.logits[:, -3, :]
            # extract logits values at tokens corresponding to letters "a", "b", "c", "d", "e"
            choice_logits = logits[:, token_ids]
            # calculate softmax of the logits and get model label predictions
            probs = F.softmax(choice_logits, dim=1)
            pred_ids = probs.argmax(-1)

            truth_token_ids = batch["input_ids"][:, -2]
            truth_label_ids = torch.tensor([
                    token_id_to_assigned_id[tid.item()] 
                    for tid in truth_token_ids
            ])

            incorrects = pred_ids != truth_label_ids
            nll = F.cross_entropy(choice_logits, truth_label_ids, reduction="none")
            
            pred_labels.append(pred_ids.cpu())
            truth_labels.append(truth_label_ids.cpu())
            nlls.append(nll[incorrects].cpu())

    pred_labels = np.concatenate(pred_labels)
    truth_labels = np.concatenate(truth_labels)
    nlls = np.concatenate(nlls)
    
    with open('mistral_outputs.npz') as f: np.savez(f, pred_labels=pred_labels, truth_labels=truth_labels, nlls=nlls)


if __name__ == '__main__':
    main()